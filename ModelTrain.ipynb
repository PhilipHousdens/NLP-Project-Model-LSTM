{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load Data",
   "id": "8f62f2d9b4e4b42d"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-28T05:17:25.647037Z",
     "start_time": "2025-02-28T05:17:23.386683Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import random\n",
    "\n",
    "# Check CUDA\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Preprocessing Function (using GPU where applicable)\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    return text\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T05:17:59.758152Z",
     "start_time": "2025-02-28T05:17:27.302592Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Load and preprocess datasets\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['article'] = df['article'].apply(preprocess_text)\n",
    "    df['highlights'] = df['highlights'].apply(preprocess_text)\n",
    "    return df['article'].tolist(), df['highlights'].tolist()\n",
    "\n",
    "train_texts, train_summaries = load_data(\"resources/train.csv\")\n",
    "val_texts, val_summaries = load_data(\"resources/validation.csv\")\n",
    "\n",
    "SAMPLE_SIZE = 15000  # Adjust based on available GPU memory\n",
    "\n",
    "# Ensure we don't sample more than available data\n",
    "train_sample_size = min(SAMPLE_SIZE, len(train_texts))\n",
    "val_sample_size = min(SAMPLE_SIZE // 4, len(val_texts))  # Use smaller validation set\n",
    "\n",
    "# Randomly sample data\n",
    "train_sample_indices = random.sample(range(len(train_texts)), train_sample_size)\n",
    "val_sample_indices = random.sample(range(len(val_texts)), val_sample_size)\n",
    "\n",
    "# Subset the dataset\n",
    "train_texts = [train_texts[i] for i in train_sample_indices]\n",
    "train_summaries = [train_summaries[i] for i in train_sample_indices]\n",
    "\n",
    "val_texts = [val_texts[i] for i in val_sample_indices]\n",
    "val_summaries = [val_summaries[i] for i in val_sample_indices]\n"
   ],
   "id": "b3c9afa29adc41be",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Build vocabulary",
   "id": "67938b9a207b4b13"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T05:35:28.428887Z",
     "start_time": "2025-02-28T05:35:16.823907Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Flatten lists into a single Counter object\n",
    "word_counts = Counter(word for text in train_texts + train_summaries for word in text.split())\n",
    "\n",
    "# Build vocabulary\n",
    "vocab = {word: i+2 for i, (word, _) in enumerate(word_counts.items())}\n",
    "vocab[\"<PAD>\"] = 0\n",
    "vocab[\"<UNK>\"] = 1\n",
    "VOCAB_SIZE = len(vocab)\n",
    "\n",
    "def load_glove_embeddings(glove_path, vocab, embed_dim=100):\n",
    "    embeddings = np.random.uniform(-0.1, 0.1, (len(vocab), embed_dim))  # Random init\n",
    "    embeddings[0] = np.zeros(embed_dim)  # <PAD> is zero vector\n",
    "    \n",
    "    with open(glove_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            split_line = line.split()\n",
    "            word, vector = split_line[0], np.array(split_line[1:], dtype=np.float32)\n",
    "            if word in vocab:\n",
    "                embeddings[vocab[word]] = vector\n",
    "\n",
    "    return torch.tensor(embeddings, dtype=torch.float32)\n",
    "\n",
    "# Load GloVe embeddings\n",
    "glove_path = \"resources/glove.6B.100d.txt\"\n",
    "EMBED_SIZE = 100\n",
    "embedding_matrix = load_glove_embeddings(glove_path, vocab, EMBED_SIZE)"
   ],
   "id": "60947aaf6cf4cb0c",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dataset Class",
   "id": "dee92db5aa04c037"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T05:38:05.594200Z",
     "start_time": "2025-02-28T05:38:05.589254Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, texts, summaries, vocab, max_len=100):\n",
    "        self.texts = [torch.tensor([vocab.get(word, 1) for word in text.split()], dtype=torch.long) for text in texts]\n",
    "        self.summaries = [torch.tensor([vocab.get(word, 1) for word in summary.split()], dtype=torch.long) for summary in summaries]\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx][:self.max_len]\n",
    "        summary = self.summaries[idx][:self.max_len]\n",
    "        return text, summary"
   ],
   "id": "281313773b68967c",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Collate Function for Padding",
   "id": "e6cdc0247f60e74c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T05:53:32.437969Z",
     "start_time": "2025-02-28T05:53:32.433931Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MAX_LEN = 128  # Set a reasonable sequence length\n",
    "\n",
    "def collate_fn(batch):\n",
    "    text, summary = zip(*batch)  # Unpack batch\n",
    "    \n",
    "    text = [torch.as_tensor(t[:MAX_LEN], dtype=torch.long).clone().detach() for t in text]\n",
    "    summary = [torch.as_tensor(s[:MAX_LEN], dtype=torch.long).clone().detach() for s in summary]\n",
    "\n",
    "    text = pad_sequence(text, batch_first=True, padding_value=0)  # Pad sequences\n",
    "    summary = pad_sequence(summary, batch_first=True, padding_value=0)\n",
    "\n",
    "    return text, summary\n",
    "\n"
   ],
   "id": "8e86937561c703a9",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define LSTM Model",
   "id": "34c9484f74d5a31e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T05:53:33.682148Z",
     "start_time": "2025-02-28T05:53:33.677369Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LSTMSeq2Seq(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, embedding_matrix, dropout=0.3):\n",
    "        super(LSTMSeq2Seq, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)  # Allows fine-tuning\n",
    "        self.encoder = nn.LSTM(embed_size, hidden_size, batch_first=True, dropout=dropout)\n",
    "        self.decoder = nn.LSTM(embed_size, hidden_size, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        embed_src = self.embedding(src)\n",
    "        embed_tgt = self.embedding(tgt)\n",
    "        _, (hidden, cell) = self.encoder(embed_src)\n",
    "        output, _ = self.decoder(embed_tgt, (hidden, cell))\n",
    "        output = self.fc(output)\n",
    "        return output\n"
   ],
   "id": "950a8877130b8868",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##  Function to generate summary",
   "id": "5dbd797747bb13c6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T05:53:36.883402Z",
     "start_time": "2025-02-28T05:53:36.875106Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import heapq\n",
    "\n",
    "def beam_search(model, text, vocab, beam_width=3, max_len=50):\n",
    "    model.eval()\n",
    "    text_tensor = torch.tensor([vocab.get(word, 1) for word in text.split()], dtype=torch.long).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(text_tensor, text_tensor)\n",
    "    \n",
    "    sequences = [([], 0)]  # (Generated sequence, score)\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        new_sequences = []\n",
    "        for seq, score in sequences:\n",
    "            last_word_idx = seq[-1] if seq else vocab[\"<PAD>\"]\n",
    "            output_probs = torch.softmax(output[0, len(seq)], dim=-1).cpu().numpy()\n",
    "\n",
    "            top_indices = np.argsort(output_probs)[-beam_width:]\n",
    "            for idx in top_indices:\n",
    "                new_sequences.append((seq + [idx], score + np.log(output_probs[idx])))\n",
    "\n",
    "        sequences = heapq.nlargest(beam_width, new_sequences, key=lambda x: x[1])\n",
    "    \n",
    "    best_sequence = sequences[0][0]\n",
    "    summary = \" \".join([list(vocab.keys())[list(vocab.values()).index(idx)] for idx in best_sequence if idx in vocab.values()])\n",
    "    return summary\n"
   ],
   "id": "1cd008bc039281b1",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T05:53:43.390909Z",
     "start_time": "2025-02-28T05:53:43.388042Z"
    }
   },
   "cell_type": "code",
   "source": [
    "EMBED_SIZE = 100\n",
    "HIDDEN_SIZE = 64\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 20\n",
    "LR = 0.001"
   ],
   "id": "5251780891645be5",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Create DataLoaders",
   "id": "3b9a910a659cb04a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T05:53:47.112069Z",
     "start_time": "2025-02-28T05:53:44.666214Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "\n",
    "# Define subset size (e.g., 50,000 samples)\n",
    "subset_size = 10000  \n",
    "train_indices = list(range(min(subset_size, len(train_texts))))\n",
    "val_indices = list(range(min(subset_size, len(val_texts))))\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_dataset = NewsDataset(train_texts, train_summaries, vocab)\n",
    "val_dataset = NewsDataset(val_texts, val_summaries, vocab)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, collate_fn=collate_fn, pin_memory=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=BATCH_SIZE, sampler=val_sampler, collate_fn=collate_fn, pin_memory=True\n",
    ")\n",
    "\n",
    "\n"
   ],
   "id": "98c8636225b02a05",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T05:53:47.768711Z",
     "start_time": "2025-02-28T05:53:47.670610Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = LSTMSeq2Seq(VOCAB_SIZE, EMBED_SIZE, HIDDEN_SIZE, embedding_matrix).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)"
   ],
   "id": "56fafcb0f69fdc64",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/philiphousden/miniconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training loop",
   "id": "eabb3c885f4ed266"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T05:53:50.676033Z",
     "start_time": "2025-02-28T05:53:50.516703Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "gc.collect()  \n",
    "torch.cuda.empty_cache()\n",
    "model = torch.compile(model)  # JIT Compilation for speedup\n",
    "\n"
   ],
   "id": "efd9178b2dffd881",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T05:53:51.627913Z",
     "start_time": "2025-02-28T05:53:51.618677Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.amp import autocast, GradScaler  # Ensure correct import\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs):\n",
    "    ACCUMULATION_STEPS = 2\n",
    "    scaler = GradScaler()  # âœ… No need for `device=\"cuda\"`\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience = 3  # Number of epochs to wait before stopping\n",
    "    counter = 0  \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for text, summary in train_loader:\n",
    "            text, summary = text.to(device), summary.to(device)\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            with torch.autocast(\"cuda\", dtype=torch.float16):\n",
    "                output = model(text, summary[:, :-1])\n",
    "                loss = criterion(output.view(-1, VOCAB_SIZE), summary[:, 1:].reshape(-1))\n",
    "    \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "    \n",
    "            train_loss += loss.item()\n",
    "    \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for text, summary in val_loader:\n",
    "                text, summary = text.to(device), summary.to(device)\n",
    "                with torch.autocast(\"cuda\", dtype=torch.float16):\n",
    "                    output = model(text, summary[:, :-1])\n",
    "                    loss = criterion(output.view(-1, VOCAB_SIZE), summary[:, 1:].reshape(-1))\n",
    "                val_loss += loss.item()\n",
    "        scheduler.step(avg_val_loss)\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "    \n",
    "        print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "        # Early stopping check\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            counter = 0  # Reset patience counter\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")  # Save best model\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(\"Early stopping triggered. Loading best model.\")\n",
    "                model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "                break  # Stop training\n"
   ],
   "id": "92b9bf81de1f8131",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-02-28T05:53:53.334748Z"
    }
   },
   "cell_type": "code",
   "source": "train_model(model, train_loader, val_loader, criterion, optimizer, EPOCHS)",
   "id": "56b00891c923729f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T05:50:30.092028Z",
     "start_time": "2025-02-28T05:50:28.869252Z"
    }
   },
   "cell_type": "code",
   "source": "beam_search(model, train_texts[0], vocab)",
   "id": "2e80a20793b4ae9b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the cantlie of the in and of 26 was report was and was 26 is new and was the new in the is been in people in a men were services in the us the he he incident was in after the and morning he the local government he the'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
