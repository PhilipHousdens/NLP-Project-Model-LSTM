{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load Data",
   "id": "8f62f2d9b4e4b42d"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-28T06:34:57.270049Z",
     "start_time": "2025-02-28T06:34:55.190645Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import random\n",
    "\n",
    "# Check CUDA\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Preprocessing Function (using GPU where applicable)\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    return text\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T06:35:26.597001Z",
     "start_time": "2025-02-28T06:34:57.274511Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Load and preprocess datasets\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['article'] = df['article'].apply(preprocess_text)\n",
    "    df['highlights'] = df['highlights'].apply(preprocess_text)\n",
    "    return df['article'].tolist(), df['highlights'].tolist()\n",
    "\n",
    "train_texts, train_summaries = load_data(\"resources/train.csv\")\n",
    "val_texts, val_summaries = load_data(\"resources/validation.csv\")\n",
    "\n",
    "SAMPLE_SIZE = 15000  # Adjust based on available GPU memory\n",
    "\n",
    "# Ensure we don't sample more than available data\n",
    "train_sample_size = min(SAMPLE_SIZE, len(train_texts))\n",
    "val_sample_size = min(SAMPLE_SIZE // 4, len(val_texts))  # Use smaller validation set\n",
    "\n",
    "# Randomly sample data\n",
    "train_sample_indices = random.sample(range(len(train_texts)), train_sample_size)\n",
    "val_sample_indices = random.sample(range(len(val_texts)), val_sample_size)\n",
    "\n",
    "# Subset the dataset\n",
    "train_texts = [train_texts[i] for i in train_sample_indices]\n",
    "train_summaries = [train_summaries[i] for i in train_sample_indices]\n",
    "\n",
    "val_texts = [val_texts[i] for i in val_sample_indices]\n",
    "val_summaries = [val_summaries[i] for i in val_sample_indices]\n"
   ],
   "id": "b3c9afa29adc41be",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Build vocabulary",
   "id": "67938b9a207b4b13"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-02-28T06:35:26.964862Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Flatten lists into a single Counter object\n",
    "word_counts = Counter(word for text in train_texts + train_summaries for word in text.split())\n",
    "vocab = {word: i+2 for i, (word, _) in enumerate(word_counts.items())}\n",
    "vocab[\"<PAD>\"] = 0\n",
    "vocab[\"<UNK>\"] = 1\n",
    "vocab[\"<SOS>\"] = len(vocab)\n",
    "vocab[\"<EOS>\"] = len(vocab) + 1\n",
    "rev_vocab = {idx: word for word, idx in vocab.items()}\n",
    "VOCAB_SIZE = len(vocab)\n",
    "\n",
    "def load_glove_embeddings(glove_path, vocab, embed_dim=100):\n",
    "    embeddings = np.random.uniform(-0.1, 0.1, (len(vocab), embed_dim))  # Random init\n",
    "    embeddings[0] = np.zeros(embed_dim)  # <PAD> is zero vector\n",
    "    \n",
    "    with open(glove_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            split_line = line.split()\n",
    "            word, vector = split_line[0], np.array(split_line[1:], dtype=np.float32)\n",
    "            if word in vocab:\n",
    "                embeddings[vocab[word]] = vector\n",
    "\n",
    "    return torch.tensor(embeddings, dtype=torch.float32)\n",
    "\n",
    "# Load GloVe embeddings\n",
    "glove_path = \"resources/glove.6B.100d.txt\"\n",
    "EMBED_SIZE = 100\n",
    "embedding_matrix = load_glove_embeddings(glove_path, vocab, EMBED_SIZE)"
   ],
   "id": "60947aaf6cf4cb0c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dataset Class",
   "id": "dee92db5aa04c037"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T05:38:05.594200Z",
     "start_time": "2025-02-28T05:38:05.589254Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, texts, summaries, vocab, max_len=100):\n",
    "        self.texts = [torch.tensor([vocab.get(word, 1) for word in text.split()], dtype=torch.long) for text in texts]\n",
    "        self.summaries = [torch.tensor([vocab.get(word, 1) for word in summary.split()], dtype=torch.long) for summary in summaries]\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx][:self.max_len]\n",
    "        summary = self.summaries[idx][:self.max_len]\n",
    "        return text, summary"
   ],
   "id": "281313773b68967c",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Collate Function for Padding",
   "id": "e6cdc0247f60e74c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T05:53:32.437969Z",
     "start_time": "2025-02-28T05:53:32.433931Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MAX_LEN = 128  # Set a reasonable sequence length\n",
    "\n",
    "def collate_fn(batch):\n",
    "    text, summary = zip(*batch)  # Unpack batch\n",
    "    \n",
    "    text = [torch.as_tensor(t[:MAX_LEN], dtype=torch.long).clone().detach() for t in text]\n",
    "    summary = [torch.as_tensor(s[:MAX_LEN], dtype=torch.long).clone().detach() for s in summary]\n",
    "\n",
    "    text = pad_sequence(text, batch_first=True, padding_value=0)  # Pad sequences\n",
    "    summary = pad_sequence(summary, batch_first=True, padding_value=0)\n",
    "\n",
    "    return text, summary\n",
    "\n"
   ],
   "id": "8e86937561c703a9",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define LSTM Model",
   "id": "34c9484f74d5a31e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T06:09:31.829018Z",
     "start_time": "2025-02-28T06:09:31.824189Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LSTMSeq2Seq(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, embedding_matrix, dropout=0.5):\n",
    "        super(LSTMSeq2Seq, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        self.encoder = nn.LSTM(embed_size, hidden_size, batch_first=True, dropout=dropout)\n",
    "        self.decoder = nn.LSTM(embed_size, hidden_size, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt, teacher_forcing_ratio=0.8):\n",
    "        embed_src = self.embedding(src)\n",
    "        embed_tgt = self.embedding(tgt)\n",
    "        \n",
    "        _, (hidden, cell) = self.encoder(embed_src)\n",
    "        \n",
    "        outputs = []\n",
    "        decoder_input = embed_tgt[:, 0].unsqueeze(1)  # Start token\n",
    "        \n",
    "        for t in range(tgt.shape[1] - 1):\n",
    "            output, (hidden, cell) = self.decoder(decoder_input, (hidden, cell))\n",
    "            output = self.fc(output)\n",
    "            outputs.append(output)\n",
    "\n",
    "            # Teacher forcing: Use true target word some of the time\n",
    "            if random.random() < teacher_forcing_ratio:\n",
    "                decoder_input = self.embedding(tgt[:, t + 1]).unsqueeze(1)\n",
    "            else:\n",
    "                decoder_input = self.embedding(torch.argmax(output, dim=-1)).detach()\n",
    "\n",
    "        return torch.cat(outputs, dim=1)\n"
   ],
   "id": "950a8877130b8868",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##  Function to generate summary",
   "id": "5dbd797747bb13c6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T06:09:33.170677Z",
     "start_time": "2025-02-28T06:09:33.165220Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import heapq\n",
    "import numpy as np\n",
    "\n",
    "def beam_search(model, text, vocab, rev_vocab, beam_width=3, max_len=50):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    text_tensor = torch.tensor([vocab.get(word, vocab[\"<UNK>\"]) for word in text.split()], dtype=torch.long).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        _, (hidden, cell) = model.encoder(model.embedding(text_tensor))\n",
    "\n",
    "    sequences = [([], 0, hidden, cell)]\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        new_sequences = []\n",
    "        for seq, score, hidden, cell in sequences:\n",
    "            last_word_idx = seq[-1] if seq else vocab[\"<SOS>\"]\n",
    "            last_word_tensor = torch.tensor([[last_word_idx]], dtype=torch.long).to(device)\n",
    "\n",
    "            embed_tgt = model.embedding(last_word_tensor)\n",
    "            output, (new_hidden, new_cell) = model.decoder(embed_tgt, (hidden, cell))\n",
    "            output_probs = torch.softmax(model.fc(output[:, -1, :]), dim=-1).cpu().numpy()\n",
    "\n",
    "            top_indices = np.argsort(output_probs[0])[-beam_width:]\n",
    "            for idx in top_indices:\n",
    "                new_sequences.append((seq + [idx], score + np.log(output_probs[0][idx]), new_hidden, new_cell))\n",
    "\n",
    "        sequences = heapq.nlargest(beam_width, new_sequences, key=lambda x: x[1])\n",
    "\n",
    "    best_sequence = sequences[0][0]\n",
    "    summary = \" \".join([rev_vocab.get(idx, \"<UNK>\") for idx in best_sequence if idx not in {vocab[\"<PAD>\"], vocab[\"<SOS>\"], vocab[\"<EOS>\"]}])\n",
    "    return summary"
   ],
   "id": "1cd008bc039281b1",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T06:09:36.531574Z",
     "start_time": "2025-02-28T06:09:36.527487Z"
    }
   },
   "cell_type": "code",
   "source": [
    "HIDDEN_SIZE = 64\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 20\n",
    "LR = 0.001"
   ],
   "id": "5251780891645be5",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Create DataLoaders",
   "id": "3b9a910a659cb04a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T06:09:40.113150Z",
     "start_time": "2025-02-28T06:09:37.653112Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "\n",
    "subset_size = 10000\n",
    "train_indices = list(range(min(subset_size, len(train_texts))))\n",
    "val_indices = list(range(min(subset_size, len(val_texts))))\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_dataset = NewsDataset(train_texts, train_summaries, vocab)\n",
    "val_dataset = NewsDataset(val_texts, val_summaries, vocab)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, collate_fn=collate_fn, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, sampler=val_sampler, collate_fn=collate_fn, pin_memory=True)"
   ],
   "id": "98c8636225b02a05",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T06:09:40.219119Z",
     "start_time": "2025-02-28T06:09:40.118463Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = LSTMSeq2Seq(VOCAB_SIZE, EMBED_SIZE, HIDDEN_SIZE, embedding_matrix).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=2)"
   ],
   "id": "56fafcb0f69fdc64",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/philiphousden/miniconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training loop",
   "id": "eabb3c885f4ed266"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T06:09:41.535920Z",
     "start_time": "2025-02-28T06:09:41.301005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "gc.collect()  \n",
    "torch.cuda.empty_cache()\n",
    "model = torch.compile(model)  # JIT Compilation for speedup\n",
    "\n"
   ],
   "id": "efd9178b2dffd881",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T06:09:42.264672Z",
     "start_time": "2025-02-28T06:09:42.256109Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=20, patience=3):\n",
    "    scaler = GradScaler()\n",
    "    best_val_loss = float(\"inf\")\n",
    "    counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        for text, summary in train_loader:\n",
    "            text, summary = text.to(device), summary.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "                output = model(text, summary)\n",
    "                loss = criterion(output.view(-1, VOCAB_SIZE), summary[:, 1:].reshape(-1))\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for text, summary in val_loader:\n",
    "                text, summary = text.to(device), summary.to(device)\n",
    "                with autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "                    output = model(text, summary)\n",
    "                    loss = criterion(output.view(-1, VOCAB_SIZE), summary[:, 1:].reshape(-1))\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        # Early Stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            counter = 0\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(\"Early stopping triggered. Loading best model.\")\n",
    "                model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "                break\n"
   ],
   "id": "92b9bf81de1f8131",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T06:21:21.859665Z",
     "start_time": "2025-02-28T06:09:44.299682Z"
    }
   },
   "cell_type": "code",
   "source": "train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, EPOCHS)",
   "id": "56b00891c923729f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 8.2608, Val Loss: 7.8390\n",
      "Epoch 2, Train Loss: 7.7774, Val Loss: 7.6463\n",
      "Epoch 3, Train Loss: 7.5800, Val Loss: 7.4855\n",
      "Epoch 4, Train Loss: 7.4157, Val Loss: 7.3792\n",
      "Epoch 5, Train Loss: 7.2843, Val Loss: 7.3024\n",
      "Epoch 6, Train Loss: 7.1773, Val Loss: 7.2414\n",
      "Epoch 7, Train Loss: 7.0863, Val Loss: 7.2019\n",
      "Epoch 8, Train Loss: 7.0082, Val Loss: 7.1691\n",
      "Epoch 9, Train Loss: 6.9409, Val Loss: 7.1496\n",
      "Epoch 10, Train Loss: 6.8811, Val Loss: 7.1258\n",
      "Epoch 11, Train Loss: 6.8223, Val Loss: 7.1040\n",
      "Epoch 12, Train Loss: 6.7625, Val Loss: 7.0789\n",
      "Epoch 13, Train Loss: 6.7048, Val Loss: 7.0543\n",
      "Epoch 14, Train Loss: 6.6510, Val Loss: 7.0408\n",
      "Epoch 15, Train Loss: 6.6010, Val Loss: 7.0281\n",
      "Epoch 16, Train Loss: 6.5557, Val Loss: 7.0126\n",
      "Epoch 17, Train Loss: 6.5131, Val Loss: 7.0041\n",
      "Epoch 18, Train Loss: 6.4723, Val Loss: 7.0011\n",
      "Epoch 19, Train Loss: 6.4351, Val Loss: 7.0004\n",
      "Epoch 20, Train Loss: 6.3979, Val Loss: 6.9902\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T06:01:21.024363Z",
     "start_time": "2025-02-28T06:01:20.252370Z"
    }
   },
   "cell_type": "code",
   "source": [
    "summary = beam_search(model, train_texts[0], vocab, rev_vocab)\n",
    "print(\"Generated Summary:\", summary)\n"
   ],
   "id": "2e80a20793b4ae9b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a cameron of a and and is was is the and on is was is year and in the year in the the been the years were the people were services and the uk have it is us is in after the and week to the new government in the'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 47
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
