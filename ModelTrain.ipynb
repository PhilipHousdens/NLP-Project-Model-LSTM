{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load Data",
   "id": "8f62f2d9b4e4b42d"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-01T04:16:58.021397Z",
     "start_time": "2025-03-01T04:16:53.610403Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from bs4 import BeautifulSoup\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import random\n",
    "\n",
    "# Check CUDA\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load Glove Embedding",
   "id": "28b39c57f0368c83"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T04:16:58.032314Z",
     "start_time": "2025-03-01T04:16:58.028372Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_glove_embeddings(glove_path, vocab, embed_dim=100):\n",
    "    embeddings = np.random.uniform(-0.1, 0.1, (len(vocab), embed_dim))  # Random init\n",
    "    embeddings[0] = np.zeros(embed_dim)  # <PAD> is zero vector\n",
    "    \n",
    "    with open(glove_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            split_line = line.split()\n",
    "            word, vector = split_line[0], np.array(split_line[1:], dtype=np.float32)\n",
    "            if word in vocab:\n",
    "                embeddings[vocab[word]] = vector\n",
    "\n",
    "    return torch.tensor(embeddings, dtype=torch.float32)"
   ],
   "id": "2e8a8b1bd51314b0",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preprocessing Functions",
   "id": "34e56e2a66c97697"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T04:16:58.362212Z",
     "start_time": "2025-03-01T04:16:58.356390Z"
    }
   },
   "cell_type": "code",
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}"
   ],
   "id": "9d8eca61a07547ea",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T04:16:58.422032Z",
     "start_time": "2025-03-01T04:16:58.418127Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_text(text):\n",
    "    newString = text.lower()\n",
    "    newString = re.sub(r'\\([^)]*\\)', '', newString)\n",
    "    newString = re.sub('\"', '', newString)\n",
    "    newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(\" \")])\n",
    "    newString = re.sub(r\"'s\\b\", \"\", newString)\n",
    "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString)\n",
    "    newString = re.sub('[m]{2,}', 'mm', newString)\n",
    "    \n",
    "    tokens = [w for w in newString.split() if w not in STOP_WORDS]\n",
    "    long_words = [i for i in tokens if len(i) > 1]  # Remove short words\n",
    "    \n",
    "    return \" \".join(long_words).strip()"
   ],
   "id": "72293ae95df84f0d",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load & Preprocess Data",
   "id": "54e04c1bf33938b4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T04:18:43.925326Z",
     "start_time": "2025-03-01T04:16:58.461653Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['article'] = df['article'].apply(preprocess_text)\n",
    "    df['highlights'] = df['highlights'].apply(preprocess_text)\n",
    "    return df['article'].tolist(), df['highlights'].tolist()\n",
    "\n",
    "train_texts, train_summaries = load_data(\"resources/train.csv\")\n",
    "val_texts, val_summaries = load_data(\"resources/validation.csv\")\n",
    "\n",
    "SAMPLE_SIZE = 10000  # Adjust based on available GPU memory\n",
    "\n",
    "# Ensure we don't sample more than available data\n",
    "train_sample_size = min(SAMPLE_SIZE, len(train_texts))\n",
    "val_sample_size = min(SAMPLE_SIZE // 4, len(val_texts))  # Use smaller validation set\n",
    "\n",
    "# Randomly sample data\n",
    "train_sample_indices = random.sample(range(len(train_texts)), train_sample_size)\n",
    "val_sample_indices = random.sample(range(len(val_texts)), val_sample_size)\n",
    "\n",
    "# Subset the dataset\n",
    "train_texts = [train_texts[i] for i in train_sample_indices]\n",
    "train_summaries = [train_summaries[i] for i in train_sample_indices]\n",
    "\n",
    "val_texts = [val_texts[i] for i in val_sample_indices]\n",
    "val_summaries = [val_summaries[i] for i in val_sample_indices]"
   ],
   "id": "e3c79e23f4f42bea",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Build Vocab",
   "id": "c4692c10e4033b45"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T04:18:44.413509Z",
     "start_time": "2025-03-01T04:18:43.949139Z"
    }
   },
   "cell_type": "code",
   "source": [
    "word_counts = Counter(word for text in train_texts + train_summaries for word in text.split())\n",
    "vocab = {word: i+2 for i, (word, _) in enumerate(word_counts.items())}\n",
    "vocab[\"<PAD>\"] = 0\n",
    "vocab[\"<UNK>\"] = 1\n",
    "vocab[\"<SOS>\"] = len(vocab)\n",
    "vocab[\"<EOS>\"] = len(vocab) + 1\n",
    "rev_vocab = {idx: word for word, idx in vocab.items()}\n",
    "VOCAB_SIZE = len(vocab)"
   ],
   "id": "731890423f29ae02",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## load GloVe Embeddings",
   "id": "9c57093ff2ddd733"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T04:18:52.991732Z",
     "start_time": "2025-03-01T04:18:44.433636Z"
    }
   },
   "cell_type": "code",
   "source": [
    "glove_path = \"resources/glove.6B.100d.txt\"\n",
    "EMBED_SIZE = 100\n",
    "embedding_matrix = load_glove_embeddings(glove_path, vocab, EMBED_SIZE)"
   ],
   "id": "89d2780690b18e94",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Find Closest Word Function",
   "id": "d3547d58f602bf15"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T04:18:53.125084Z",
     "start_time": "2025-03-01T04:18:53.009711Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from difflib import get_close_matches\n",
    "import numpy as np\n",
    "\n",
    "def find_closest_word(word, vocab, embedding_matrix, top_n=5):\n",
    "    if word in vocab:\n",
    "        return word  # Word is already in vocab\n",
    "\n",
    "    # Try to find a similar word based on spelling (Levenshtein-like match)\n",
    "    close_matches = get_close_matches(word, vocab.keys(), n=1, cutoff=0.7)\n",
    "    if close_matches:\n",
    "        return close_matches[0]  # Return best spelling correction match\n",
    "\n",
    "    # Otherwise, fallback to <UNK> embedding (this is what caused \"mail\" issue before)\n",
    "    word_index = vocab[\"<UNK>\"]\n",
    "    word_vector = np.array(embedding_matrix[word_index], dtype=np.float32)\n",
    "\n",
    "    # Normalize word vector\n",
    "    word_norm = np.linalg.norm(word_vector)\n",
    "    if word_norm > 1e-10:\n",
    "        word_vector /= word_norm\n",
    "\n",
    "    # Normalize entire embedding matrix\n",
    "    embedding_norms = np.linalg.norm(embedding_matrix, axis=1, keepdims=True)\n",
    "    normalized_embeddings = embedding_matrix / np.maximum(embedding_norms, 1e-10)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    similarities = np.dot(normalized_embeddings, word_vector)\n",
    "\n",
    "    # Sort words by highest similarity\n",
    "    sorted_indices = np.argsort(similarities)[::-1]  # Descending order\n",
    "\n",
    "    # Return the best match, avoiding <PAD> and <UNK>\n",
    "    for idx in sorted_indices:\n",
    "        candidate_word = list(vocab.keys())[idx]\n",
    "        if candidate_word not in [\"<PAD>\", \"<UNK>\"]:\n",
    "            return candidate_word\n",
    "\n",
    "# Example usage\n",
    "closest_word = find_closest_word(\"wolrd\", vocab, embedding_matrix)\n",
    "print(\"\\nFinal Closest Match:\", closest_word)\n"
   ],
   "id": "6b623906eea9824c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Closest Match: word\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dataset Class",
   "id": "dee92db5aa04c037"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T04:18:53.149548Z",
     "start_time": "2025-03-01T04:18:53.145193Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, texts, summaries, vocab, max_len=100):\n",
    "        self.texts = [\n",
    "            torch.tensor(\n",
    "                [vocab.get(word, vocab.get(find_closest_word(word, vocab, embedding_matrix), 1)) for word in text.split()], \n",
    "                dtype=torch.long\n",
    "            ) for text in texts\n",
    "        ]\n",
    "        self.summaries = [\n",
    "            torch.tensor(\n",
    "                [vocab.get(word, vocab.get(find_closest_word(word, vocab, embedding_matrix), 1)) for word in summary.split()], \n",
    "                dtype=torch.long\n",
    "            ) for summary in summaries\n",
    "        ]\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx][:self.max_len]\n",
    "        summary = self.summaries[idx][:self.max_len]\n",
    "        return text, summary\n"
   ],
   "id": "281313773b68967c",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Collate Function for Padding",
   "id": "e6cdc0247f60e74c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T04:18:53.203211Z",
     "start_time": "2025-03-01T04:18:53.196956Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MAX_LEN = 75  # Set a reasonable sequence length\n",
    "\n",
    "def collate_fn(batch):\n",
    "    text, summary = zip(*batch)  # Unpack batch\n",
    "    \n",
    "    text = [torch.as_tensor(t[:MAX_LEN], dtype=torch.long).clone().detach() for t in text]\n",
    "    summary = [torch.as_tensor(s[:MAX_LEN], dtype=torch.long).clone().detach() for s in summary]\n",
    "\n",
    "    text = pad_sequence(text, batch_first=True, padding_value=0)  # Pad sequences\n",
    "    summary = pad_sequence(summary, batch_first=True, padding_value=0)\n",
    "\n",
    "    return text, summary\n",
    "\n"
   ],
   "id": "8e86937561c703a9",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define LSTM Model",
   "id": "34c9484f74d5a31e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T04:18:53.268408Z",
     "start_time": "2025-03-01T04:18:53.258892Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LSTMSeq2Seq(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, embedding_matrix, dropout=0.3):\n",
    "        super(LSTMSeq2Seq, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        self.encoder = nn.LSTM(embed_size, hidden_size, batch_first=True, dropout=dropout)\n",
    "        self.decoder = nn.LSTM(embed_size, hidden_size, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
    "        embed_src = self.embedding(src)\n",
    "        embed_tgt = self.embedding(tgt)\n",
    "        \n",
    "        _, (hidden, cell) = self.encoder(embed_src)\n",
    "        \n",
    "        outputs = []\n",
    "        decoder_input = embed_tgt[:, 0].unsqueeze(1)  # Start token\n",
    "        \n",
    "        for t in range(tgt.shape[1] - 1):\n",
    "            output, (hidden, cell) = self.decoder(decoder_input, (hidden, cell))\n",
    "            output = self.fc(output)\n",
    "            outputs.append(output)\n",
    "\n",
    "            # Teacher forcing: Use true target word some of the time\n",
    "            if random.random() < teacher_forcing_ratio:\n",
    "                decoder_input = self.embedding(tgt[:, t + 1]).unsqueeze(1)\n",
    "            else:\n",
    "                decoder_input = self.embedding(torch.argmax(output, dim=-1)).detach()\n",
    "\n",
    "        return torch.cat(outputs, dim=1)\n"
   ],
   "id": "950a8877130b8868",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##  Function to generate summary",
   "id": "5dbd797747bb13c6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T04:18:53.335666Z",
     "start_time": "2025-03-01T04:18:53.326800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import heapq\n",
    "import numpy as np\n",
    "\n",
    "def beam_search(model, text, vocab, rev_vocab, beam_width=3, max_len=50):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Convert text to indices\n",
    "        text_indices = [vocab.get(word, vocab[\"<UNK>\"]) for word in text.split()]\n",
    "        text_tensor = torch.tensor(text_indices, dtype=torch.long).unsqueeze(0).to(device)  # (1, seq_len)\n",
    "\n",
    "        # Encode input text\n",
    "        embedded_text = model.embedding(text_tensor)\n",
    "        _, (hidden, cell) = model.encoder(embedded_text)  # Encoder output states\n",
    "\n",
    "        # Initialize beam search: (score, sequence, hidden state, cell state)\n",
    "        sequences = [(0, [vocab[\"<SOS>\"]], hidden, cell)]\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            all_candidates = []\n",
    "            for score, seq, h, c in sequences:\n",
    "                last_word = seq[-1]\n",
    "\n",
    "                # Stop expanding sequences that already reached <EOS>\n",
    "                if last_word == vocab[\"<EOS>\"]:\n",
    "                    all_candidates.append((score, seq, h, c))\n",
    "                    continue\n",
    "\n",
    "                # Convert last word to tensor\n",
    "                last_word_tensor = torch.tensor([last_word], dtype=torch.long).to(device)\n",
    "                embedded_input = model.embedding(last_word_tensor).unsqueeze(1)  # (1, 1, embed_size)\n",
    "\n",
    "                # Decoder step\n",
    "                output, (new_h, new_c) = model.decoder(embedded_input, (h, c))\n",
    "\n",
    "                # Compute probabilities and get top-k words\n",
    "                output_probs = torch.softmax(model.fc(output.squeeze(1)), dim=-1)  # (1, vocab_size)\n",
    "                topk_probs, topk_indices = torch.topk(output_probs, beam_width, dim=-1)  # (1, beam_width)\n",
    "\n",
    "                # Add new candidates to the list\n",
    "                for i in range(beam_width):\n",
    "                    word_idx = topk_indices[0][i].item()\n",
    "                    word_prob = topk_probs[0][i].item()\n",
    "\n",
    "                    candidate = (score + np.log(word_prob), seq + [word_idx], new_h, new_c)\n",
    "                    all_candidates.append(candidate)\n",
    "\n",
    "            # Keep top `beam_width` sequences\n",
    "            sequences = sorted(all_candidates, key=lambda x: x[0], reverse=True)[:beam_width]\n",
    "\n",
    "        # Select the best sequence\n",
    "        best_seq = sequences[0][1]\n",
    "\n",
    "        # Convert indices to words\n",
    "        summary = \" \".join([find_closest_word(rev_vocab.get(idx, \"<UNK>\"), vocab, embedding_matrix) if idx == vocab[\"<UNK>\"] else rev_vocab[idx] for idx in best_seq if idx not in {vocab[\"<SOS>\"], vocab[\"<EOS>\"], vocab[\"<PAD>\"]}])\n",
    "\n",
    "        return summary\n"
   ],
   "id": "1cd008bc039281b1",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T04:18:53.403251Z",
     "start_time": "2025-03-01T04:18:53.397987Z"
    }
   },
   "cell_type": "code",
   "source": [
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "LR = 0.001"
   ],
   "id": "5251780891645be5",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Create DataLoaders",
   "id": "3b9a910a659cb04a"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-03-01T04:18:53.448601Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "\n",
    "subset_size = 5000\n",
    "train_indices = list(range(min(subset_size, len(train_texts))))\n",
    "val_indices = list(range(min(subset_size, len(val_texts))))\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_dataset = NewsDataset(train_texts, train_summaries, vocab)\n",
    "val_dataset = NewsDataset(val_texts, val_summaries, vocab)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, collate_fn=collate_fn, pin_memory=True, num_workers=4, prefetch_factor=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, sampler=val_sampler, collate_fn=collate_fn, pin_memory=True, num_workers=4, prefetch_factor=2)"
   ],
   "id": "98c8636225b02a05",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T04:16:49.313569400Z",
     "start_time": "2025-03-01T04:06:32.030086Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = LSTMSeq2Seq(VOCAB_SIZE, EMBED_SIZE, HIDDEN_SIZE, embedding_matrix).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=2)"
   ],
   "id": "56fafcb0f69fdc64",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/philiphousden/miniconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training loop",
   "id": "eabb3c885f4ed266"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T04:16:49.313569400Z",
     "start_time": "2025-03-01T04:06:33.362070Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "gc.collect()  \n",
    "torch.cuda.empty_cache()\n",
    "model = torch.compile(model)  # JIT Compilation for speedup\n",
    "\n"
   ],
   "id": "efd9178b2dffd881",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T04:16:49.313569400Z",
     "start_time": "2025-03-01T04:06:53.851763Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=10, patience=3):\n",
    "    scaler = torch.amp.GradScaler()\n",
    "    best_val_loss = float(\"inf\")\n",
    "    counter = 0\n",
    "    accumulation_steps = 4  # Accumulate gradients over 4 batches before updating\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        for i, (text, summary) in enumerate(train_loader):  # Use enumerate() here!\n",
    "            text, summary = text.to(device), summary.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.amp.autocast(\"cuda\", dtype=torch.float16):\n",
    "                output = model(text, summary)\n",
    "                loss = criterion(output.view(-1, VOCAB_SIZE).to(device), summary[:, 1:].reshape(-1).to(device))\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if (i + 1) % accumulation_steps == 0:  # Only update every N steps\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation Step\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for text, summary in val_loader:\n",
    "                text, summary = text.to(device), summary.to(device)\n",
    "                with torch.amp.autocast(\"cuda\", dtype=torch.float16):\n",
    "                    output = model(text, summary)\n",
    "                    loss = criterion(output.view(-1, VOCAB_SIZE), summary[:, 1:].reshape(-1))\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        # Early Stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            counter = 0\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(\"Early stopping triggered. Loading best model.\")\n",
    "                model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "                break\n"
   ],
   "id": "92b9bf81de1f8131",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T04:16:49.313569400Z",
     "start_time": "2025-03-01T04:06:55.682581Z"
    }
   },
   "cell_type": "code",
   "source": "train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, EPOCHS)",
   "id": "56b00891c923729f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 10.4218, Val Loss: 9.4473\n",
      "Epoch 2, Train Loss: 9.3080, Val Loss: 9.3451\n",
      "Epoch 3, Train Loss: 9.2178, Val Loss: 9.2916\n",
      "Epoch 4, Train Loss: 9.1654, Val Loss: 9.2866\n",
      "Epoch 5, Train Loss: 9.1421, Val Loss: 9.2685\n",
      "Epoch 6, Train Loss: 9.1260, Val Loss: 9.2752\n",
      "Epoch 7, Train Loss: 9.1190, Val Loss: 9.2733\n",
      "Epoch 8, Train Loss: 9.1096, Val Loss: 9.2748\n",
      "Early stopping triggered. Loading best model.\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T04:16:49.313569400Z",
     "start_time": "2025-03-01T04:09:54.036969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "summary = beam_search(model, train_texts[0], vocab, rev_vocab, beam_width=5, max_len=75)\n",
    "print(\"Generated Summary:\", summary)\n"
   ],
   "id": "2e80a20793b4ae9b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Summary: says says says says says says says says says says says says says says says says says says says says says says says says says says says says says says says says says says says says says says says says says says says says says says says says says says says says says says says says says says says says says says says says says says says says says says says says says says says\n"
     ]
    }
   ],
   "execution_count": 44
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
