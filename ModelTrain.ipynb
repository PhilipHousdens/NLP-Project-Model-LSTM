{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load Data",
   "id": "8f62f2d9b4e4b42d"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-28T16:51:55.847107Z",
     "start_time": "2025-02-28T16:51:54.828Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from bs4 import BeautifulSoup\n",
    "from spacy.lang.en import stop_words\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import random\n",
    "\n",
    "# Check CUDA\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T16:52:23.419809Z",
     "start_time": "2025-02-28T16:51:56.392715Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Load and preprocess datasets\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['article'] = df['article'].apply(preprocess_text)\n",
    "    df['highlights'] = df['highlights'].apply(preprocess_text)\n",
    "    return df['article'].tolist(), df['highlights'].tolist()\n",
    "\n",
    "train_texts, train_summaries = load_data(\"resources/train.csv\")\n",
    "val_texts, val_summaries = load_data(\"resources/validation.csv\")\n",
    "\n",
    "SAMPLE_SIZE = 10000  # Adjust based on available GPU memory\n",
    "\n",
    "# Ensure we don't sample more than available data\n",
    "train_sample_size = min(SAMPLE_SIZE, len(train_texts))\n",
    "val_sample_size = min(SAMPLE_SIZE // 4, len(val_texts))  # Use smaller validation set\n",
    "\n",
    "# Randomly sample data\n",
    "train_sample_indices = random.sample(range(len(train_texts)), train_sample_size)\n",
    "val_sample_indices = random.sample(range(len(val_texts)), val_sample_size)\n",
    "\n",
    "# Subset the dataset\n",
    "train_texts = [train_texts[i] for i in train_sample_indices]\n",
    "train_summaries = [train_summaries[i] for i in train_sample_indices]\n",
    "\n",
    "val_texts = [val_texts[i] for i in val_sample_indices]\n",
    "val_summaries = [val_summaries[i] for i in val_sample_indices]\n"
   ],
   "id": "b3c9afa29adc41be",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Build vocabulary",
   "id": "67938b9a207b4b13"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T16:52:36.169311Z",
     "start_time": "2025-02-28T16:52:26.785087Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Flatten lists into a single Counter object\n",
    "word_counts = Counter(word for text in train_texts + train_summaries for word in text.split())\n",
    "vocab = {word: i+2 for i, (word, _) in enumerate(word_counts.items())}\n",
    "vocab[\"<PAD>\"] = 0\n",
    "vocab[\"<UNK>\"] = 1\n",
    "vocab[\"<SOS>\"] = len(vocab)\n",
    "vocab[\"<EOS>\"] = len(vocab) + 1\n",
    "rev_vocab = {idx: word for word, idx in vocab.items()}\n",
    "VOCAB_SIZE = len(vocab)\n",
    "\n",
    "def load_glove_embeddings(glove_path, vocab, embed_dim=100):\n",
    "    embeddings = np.random.uniform(-0.1, 0.1, (len(vocab), embed_dim))  # Random init\n",
    "    embeddings[0] = np.zeros(embed_dim)  # <PAD> is zero vector\n",
    "    \n",
    "    with open(glove_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            split_line = line.split()\n",
    "            word, vector = split_line[0], np.array(split_line[1:], dtype=np.float32)\n",
    "            if word in vocab:\n",
    "                embeddings[vocab[word]] = vector\n",
    "\n",
    "    return torch.tensor(embeddings, dtype=torch.float32)\n",
    "\n",
    "# Load GloVe embeddings\n",
    "glove_path = \"resources/glove.6B.100d.txt\"\n",
    "EMBED_SIZE = 100\n",
    "embedding_matrix = load_glove_embeddings(glove_path, vocab, EMBED_SIZE)"
   ],
   "id": "60947aaf6cf4cb0c",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T16:52:37.089758Z",
     "start_time": "2025-02-28T16:52:37.084934Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Preprocessing Function (using GPU where applicable)\n",
    "def preprocess_text(text,num):\n",
    "    newString = text.lower()\n",
    "    newString = BeautifulSoup(newString, \"lxml\").text\n",
    "    newString = re.sub(r'\\([^)]*\\)', '', newString)\n",
    "    newString = re.sub('\"','', newString)\n",
    "    newString = ' '.join([embedding_matrix[t] if t in embedding_matrix else t for t in newString.split(\" \")])    \n",
    "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
    "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString) \n",
    "    newString = re.sub('[m]{2,}', 'mm', newString)\n",
    "    if(num==0):\n",
    "        tokens = [w for w in newString.split() if not w in stop_words]\n",
    "    else:\n",
    "        tokens=newString.split()\n",
    "    long_words=[]\n",
    "    for i in tokens:\n",
    "        if len(i)>1:                                                 #removing short word\n",
    "            long_words.append(i)   \n",
    "    return (\" \".join(long_words)).strip()"
   ],
   "id": "5d22f55e9cf9d100",
   "outputs": [],
   "execution_count": 63
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dataset Class",
   "id": "dee92db5aa04c037"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T16:52:40.299472Z",
     "start_time": "2025-02-28T16:52:40.293970Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, texts, summaries, vocab, max_len=100):\n",
    "        self.texts = [torch.tensor([vocab.get(word, 1) for word in text.split()], dtype=torch.long) for text in texts]\n",
    "        self.summaries = [torch.tensor([vocab.get(word, 1) for word in summary.split()], dtype=torch.long) for summary in summaries]\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx][:self.max_len]\n",
    "        summary = self.summaries[idx][:self.max_len]\n",
    "        return text, summary"
   ],
   "id": "281313773b68967c",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Collate Function for Padding",
   "id": "e6cdc0247f60e74c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T16:52:41.501441Z",
     "start_time": "2025-02-28T16:52:41.496126Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MAX_LEN = 75  # Set a reasonable sequence length\n",
    "\n",
    "def collate_fn(batch):\n",
    "    text, summary = zip(*batch)  # Unpack batch\n",
    "    \n",
    "    text = [torch.as_tensor(t[:MAX_LEN], dtype=torch.long).clone().detach() for t in text]\n",
    "    summary = [torch.as_tensor(s[:MAX_LEN], dtype=torch.long).clone().detach() for s in summary]\n",
    "\n",
    "    text = pad_sequence(text, batch_first=True, padding_value=0)  # Pad sequences\n",
    "    summary = pad_sequence(summary, batch_first=True, padding_value=0)\n",
    "\n",
    "    return text, summary\n",
    "\n"
   ],
   "id": "8e86937561c703a9",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define LSTM Model",
   "id": "34c9484f74d5a31e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T16:52:43.460052Z",
     "start_time": "2025-02-28T16:52:43.453500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LSTMSeq2Seq(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, embedding_matrix, dropout=0.3):\n",
    "        super(LSTMSeq2Seq, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        self.encoder = nn.LSTM(embed_size, hidden_size, batch_first=True, dropout=dropout)\n",
    "        self.decoder = nn.LSTM(embed_size, hidden_size, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
    "        embed_src = self.embedding(src)\n",
    "        embed_tgt = self.embedding(tgt)\n",
    "        \n",
    "        _, (hidden, cell) = self.encoder(embed_src)\n",
    "        \n",
    "        outputs = []\n",
    "        decoder_input = embed_tgt[:, 0].unsqueeze(1)  # Start token\n",
    "        \n",
    "        for t in range(tgt.shape[1] - 1):\n",
    "            output, (hidden, cell) = self.decoder(decoder_input, (hidden, cell))\n",
    "            output = self.fc(output)\n",
    "            outputs.append(output)\n",
    "\n",
    "            # Teacher forcing: Use true target word some of the time\n",
    "            if random.random() < teacher_forcing_ratio:\n",
    "                decoder_input = self.embedding(tgt[:, t + 1]).unsqueeze(1)\n",
    "            else:\n",
    "                decoder_input = self.embedding(torch.argmax(output, dim=-1)).detach()\n",
    "\n",
    "        return torch.cat(outputs, dim=1)\n"
   ],
   "id": "950a8877130b8868",
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##  Function to generate summary",
   "id": "5dbd797747bb13c6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T16:58:42.371403Z",
     "start_time": "2025-02-28T16:58:42.360899Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import heapq\n",
    "import numpy as np\n",
    "\n",
    "def beam_search(model, text, vocab, rev_vocab, beam_width=3, max_len=50):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Convert text to indices\n",
    "        text_indices = [vocab.get(word, vocab[\"<UNK>\"]) for word in text.split()]\n",
    "        text_tensor = torch.tensor(text_indices, dtype=torch.long).unsqueeze(0).to(device)  # (1, seq_len)\n",
    "\n",
    "        # Encode input text\n",
    "        embedded_text = model.embedding(text_tensor)\n",
    "        _, (hidden, cell) = model.encoder(embedded_text)  # Encoder output states\n",
    "\n",
    "        # Initialize beam search: (score, sequence, hidden state, cell state)\n",
    "        sequences = [(0, [vocab[\"<SOS>\"]], hidden, cell)]\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            all_candidates = []\n",
    "            for score, seq, h, c in sequences:\n",
    "                last_word = seq[-1]\n",
    "\n",
    "                # Stop expanding sequences that already reached <EOS>\n",
    "                if last_word == vocab[\"<EOS>\"]:\n",
    "                    all_candidates.append((score, seq, h, c))\n",
    "                    continue\n",
    "\n",
    "                # Convert last word to tensor\n",
    "                last_word_tensor = torch.tensor([last_word], dtype=torch.long).to(device)\n",
    "                embedded_input = model.embedding(last_word_tensor).unsqueeze(1)  # (1, 1, embed_size)\n",
    "\n",
    "                # Decoder step\n",
    "                output, (new_h, new_c) = model.decoder(embedded_input, (h, c))\n",
    "\n",
    "                # Compute probabilities and get top-k words\n",
    "                output_probs = torch.softmax(model.fc(output.squeeze(1)), dim=-1)  # (1, vocab_size)\n",
    "                topk_probs, topk_indices = torch.topk(output_probs, beam_width, dim=-1)  # (1, beam_width)\n",
    "\n",
    "                # Add new candidates to the list\n",
    "                for i in range(beam_width):\n",
    "                    word_idx = topk_indices[0][i].item()\n",
    "                    word_prob = topk_probs[0][i].item()\n",
    "\n",
    "                    candidate = (score + np.log(word_prob), seq + [word_idx], new_h, new_c)\n",
    "                    all_candidates.append(candidate)\n",
    "\n",
    "            # Keep top `beam_width` sequences\n",
    "            sequences = sorted(all_candidates, key=lambda x: x[0], reverse=True)[:beam_width]\n",
    "\n",
    "        # Select the best sequence\n",
    "        best_seq = sequences[0][1]\n",
    "\n",
    "        # Convert indices to words\n",
    "        summary = \" \".join([rev_vocab[idx] for idx in best_seq if idx not in {vocab[\"<SOS>\"], vocab[\"<EOS>\"], vocab[\"<PAD>\"]}])\n",
    "        return summary\n"
   ],
   "id": "1cd008bc039281b1",
   "outputs": [],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T16:52:47.129067Z",
     "start_time": "2025-02-28T16:52:47.125950Z"
    }
   },
   "cell_type": "code",
   "source": [
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "LR = 0.001"
   ],
   "id": "5251780891645be5",
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Create DataLoaders",
   "id": "3b9a910a659cb04a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T16:52:50.350319Z",
     "start_time": "2025-02-28T16:52:48.382599Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "\n",
    "subset_size = 5000\n",
    "train_indices = list(range(min(subset_size, len(train_texts))))\n",
    "val_indices = list(range(min(subset_size, len(val_texts))))\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_dataset = NewsDataset(train_texts, train_summaries, vocab)\n",
    "val_dataset = NewsDataset(val_texts, val_summaries, vocab)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, collate_fn=collate_fn, pin_memory=True, num_workers=4, prefetch_factor=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, sampler=val_sampler, collate_fn=collate_fn, pin_memory=True, num_workers=4, prefetch_factor=2)"
   ],
   "id": "98c8636225b02a05",
   "outputs": [],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T16:52:50.964654Z",
     "start_time": "2025-02-28T16:52:50.838363Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = LSTMSeq2Seq(VOCAB_SIZE, EMBED_SIZE, HIDDEN_SIZE, embedding_matrix).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=2)"
   ],
   "id": "56fafcb0f69fdc64",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/philiphousden/miniconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training loop",
   "id": "eabb3c885f4ed266"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T16:52:52.736633Z",
     "start_time": "2025-02-28T16:52:52.546013Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "gc.collect()  \n",
    "torch.cuda.empty_cache()\n",
    "model = torch.compile(model)  # JIT Compilation for speedup\n",
    "\n"
   ],
   "id": "efd9178b2dffd881",
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T16:52:53.830950Z",
     "start_time": "2025-02-28T16:52:53.824721Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=20, patience=3):\n",
    "    scaler = torch.amp.GradScaler()\n",
    "    best_val_loss = float(\"inf\")\n",
    "    counter = 0\n",
    "    accumulation_steps = 4  # Accumulate gradients over 4 batches before updating\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        for i, (text, summary) in enumerate(train_loader):  # Use enumerate() here!\n",
    "            text, summary = text.to(device), summary.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.amp.autocast(\"cuda\", dtype=torch.float16):\n",
    "                output = model(text, summary)\n",
    "                loss = criterion(output.view(-1, VOCAB_SIZE).to(device), summary[:, 1:].reshape(-1).to(device))\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if (i + 1) % accumulation_steps == 0:  # Only update every N steps\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation Step\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for text, summary in val_loader:\n",
    "                text, summary = text.to(device), summary.to(device)\n",
    "                with torch.amp.autocast(\"cuda\", dtype=torch.float16):\n",
    "                    output = model(text, summary)\n",
    "                    loss = criterion(output.view(-1, VOCAB_SIZE), summary[:, 1:].reshape(-1))\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        # Early Stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            counter = 0\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(\"Early stopping triggered. Loading best model.\")\n",
    "                model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "                break\n"
   ],
   "id": "92b9bf81de1f8131",
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T16:58:42.298673Z",
     "start_time": "2025-02-28T16:52:55.554424Z"
    }
   },
   "cell_type": "code",
   "source": "train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, EPOCHS)",
   "id": "56b00891c923729f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 10.2410, Val Loss: 8.2275\n",
      "Epoch 2, Train Loss: 8.1067, Val Loss: 8.0849\n",
      "Epoch 3, Train Loss: 8.0522, Val Loss: 8.0270\n",
      "Epoch 4, Train Loss: 7.9972, Val Loss: 7.9976\n",
      "Epoch 5, Train Loss: 7.9738, Val Loss: 7.9823\n",
      "Epoch 6, Train Loss: 7.9549, Val Loss: 7.9756\n",
      "Epoch 7, Train Loss: 7.9407, Val Loss: 7.9726\n",
      "Epoch 8, Train Loss: 7.9311, Val Loss: 7.9713\n",
      "Epoch 9, Train Loss: 7.9202, Val Loss: 7.9627\n",
      "Epoch 10, Train Loss: 7.9135, Val Loss: 7.9643\n"
     ]
    }
   ],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T16:58:49.176001Z",
     "start_time": "2025-02-28T16:58:48.495708Z"
    }
   },
   "cell_type": "code",
   "source": [
    "summary = beam_search(model, train_texts[0], vocab, rev_vocab, beam_width=5, max_len=75)\n",
    "print(\"Generated Summary:\", summary)\n"
   ],
   "id": "2e80a20793b4ae9b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Summary: was to to the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n"
     ]
    }
   ],
   "execution_count": 76
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
