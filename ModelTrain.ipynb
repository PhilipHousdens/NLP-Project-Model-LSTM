{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load Data",
   "id": "8f62f2d9b4e4b42d"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-01T03:53:03.677412Z",
     "start_time": "2025-03-01T03:53:03.672877Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from bs4 import BeautifulSoup\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import random\n",
    "\n",
    "# Check CUDA\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load Glove Embedding",
   "id": "28b39c57f0368c83"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T03:53:04.704704Z",
     "start_time": "2025-03-01T03:53:04.700252Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_glove_embeddings(glove_path, vocab, embed_dim=100):\n",
    "    embeddings = np.random.uniform(-0.1, 0.1, (len(vocab), embed_dim))  # Random init\n",
    "    embeddings[0] = np.zeros(embed_dim)  # <PAD> is zero vector\n",
    "    \n",
    "    with open(glove_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            split_line = line.split()\n",
    "            word, vector = split_line[0], np.array(split_line[1:], dtype=np.float32)\n",
    "            if word in vocab:\n",
    "                embeddings[vocab[word]] = vector\n",
    "\n",
    "    return torch.tensor(embeddings, dtype=torch.float32)"
   ],
   "id": "2e8a8b1bd51314b0",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preprocessing Functions",
   "id": "34e56e2a66c97697"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T04:01:23.185846Z",
     "start_time": "2025-03-01T04:01:23.181825Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_text(text):\n",
    "    newString = text.lower()\n",
    "    newString = re.sub(r'\\([^)]*\\)', '', newString)\n",
    "    newString = re.sub('\"', '', newString)\n",
    "    newString = re.sub(r\"'s\\b\", \"\", newString)\n",
    "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString)\n",
    "    newString = re.sub('[m]{2,}', 'mm', newString)\n",
    "    \n",
    "    tokens = [w for w in newString.split() if w not in STOP_WORDS]\n",
    "    long_words = [i for i in tokens if len(i) > 1]  # Remove short words\n",
    "    \n",
    "    return \" \".join(long_words).strip()"
   ],
   "id": "72293ae95df84f0d",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load & Preprocess Data",
   "id": "54e04c1bf33938b4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T04:02:48.537232Z",
     "start_time": "2025-03-01T04:01:24.938884Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['article'] = df['article'].apply(preprocess_text)\n",
    "    df['highlights'] = df['highlights'].apply(preprocess_text)\n",
    "    return df['article'].tolist(), df['highlights'].tolist()\n",
    "\n",
    "train_texts, train_summaries = load_data(\"resources/train.csv\")\n",
    "val_texts, val_summaries = load_data(\"resources/validation.csv\")\n",
    "\n",
    "SAMPLE_SIZE = 10000  # Adjust based on available GPU memory\n",
    "\n",
    "# Ensure we don't sample more than available data\n",
    "train_sample_size = min(SAMPLE_SIZE, len(train_texts))\n",
    "val_sample_size = min(SAMPLE_SIZE // 4, len(val_texts))  # Use smaller validation set\n",
    "\n",
    "# Randomly sample data\n",
    "train_sample_indices = random.sample(range(len(train_texts)), train_sample_size)\n",
    "val_sample_indices = random.sample(range(len(val_texts)), val_sample_size)\n",
    "\n",
    "# Subset the dataset\n",
    "train_texts = [train_texts[i] for i in train_sample_indices]\n",
    "train_summaries = [train_summaries[i] for i in train_sample_indices]\n",
    "\n",
    "val_texts = [val_texts[i] for i in val_sample_indices]\n",
    "val_summaries = [val_summaries[i] for i in val_sample_indices]"
   ],
   "id": "e3c79e23f4f42bea",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Build Vocab",
   "id": "c4692c10e4033b45"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T04:02:51.835271Z",
     "start_time": "2025-03-01T04:02:51.260785Z"
    }
   },
   "cell_type": "code",
   "source": [
    "word_counts = Counter(word for text in train_texts + train_summaries for word in text.split())\n",
    "vocab = {word: i+2 for i, (word, _) in enumerate(word_counts.items())}\n",
    "vocab[\"<PAD>\"] = 0\n",
    "vocab[\"<UNK>\"] = 1\n",
    "vocab[\"<SOS>\"] = len(vocab)\n",
    "vocab[\"<EOS>\"] = len(vocab) + 1\n",
    "rev_vocab = {idx: word for word, idx in vocab.items()}\n",
    "VOCAB_SIZE = len(vocab)"
   ],
   "id": "731890423f29ae02",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## load GloVe Embeddings",
   "id": "9c57093ff2ddd733"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T04:03:01.597908Z",
     "start_time": "2025-03-01T04:02:52.761399Z"
    }
   },
   "cell_type": "code",
   "source": [
    "glove_path = \"resources/glove.6B.100d.txt\"\n",
    "EMBED_SIZE = 100\n",
    "embedding_matrix = load_glove_embeddings(glove_path, vocab, EMBED_SIZE)"
   ],
   "id": "89d2780690b18e94",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Find Closest Word Function",
   "id": "d3547d58f602bf15"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T04:05:54.438371Z",
     "start_time": "2025-03-01T04:05:54.344042Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from difflib import get_close_matches\n",
    "import numpy as np\n",
    "\n",
    "def find_closest_word(word, vocab, embedding_matrix, top_n=5):\n",
    "    if word in vocab:\n",
    "        return word  # Word is already in vocab\n",
    "\n",
    "    # Try to find a similar word based on spelling (Levenshtein-like match)\n",
    "    close_matches = get_close_matches(word, vocab.keys(), n=1, cutoff=0.7)\n",
    "    if close_matches:\n",
    "        return close_matches[0]  # Return best spelling correction match\n",
    "\n",
    "    # Otherwise, fallback to <UNK> embedding (this is what caused \"mail\" issue before)\n",
    "    word_index = vocab[\"<UNK>\"]\n",
    "    word_vector = np.array(embedding_matrix[word_index], dtype=np.float32)\n",
    "\n",
    "    # Normalize word vector\n",
    "    word_norm = np.linalg.norm(word_vector)\n",
    "    if word_norm > 1e-10:\n",
    "        word_vector /= word_norm\n",
    "\n",
    "    # Normalize entire embedding matrix\n",
    "    embedding_norms = np.linalg.norm(embedding_matrix, axis=1, keepdims=True)\n",
    "    normalized_embeddings = embedding_matrix / np.maximum(embedding_norms, 1e-10)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    similarities = np.dot(normalized_embeddings, word_vector)\n",
    "\n",
    "    # Sort words by highest similarity\n",
    "    sorted_indices = np.argsort(similarities)[::-1]  # Descending order\n",
    "\n",
    "    # Return the best match, avoiding <PAD> and <UNK>\n",
    "    for idx in sorted_indices:\n",
    "        candidate_word = list(vocab.keys())[idx]\n",
    "        if candidate_word not in [\"<PAD>\", \"<UNK>\"]:\n",
    "            return candidate_word\n",
    "\n",
    "# Example usage\n",
    "closest_word = find_closest_word(\"hapy\", vocab, embedding_matrix)\n",
    "print(\"\\nFinal Closest Match:\", closest_word)\n"
   ],
   "id": "6b623906eea9824c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Closest Match: happy\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dataset Class",
   "id": "dee92db5aa04c037"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T04:06:14.720373Z",
     "start_time": "2025-03-01T04:06:14.715888Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, texts, summaries, vocab, max_len=100):\n",
    "        self.texts = [torch.tensor([vocab.get(word, 1) for word in text.split()], dtype=torch.long) for text in texts]\n",
    "        self.summaries = [torch.tensor([vocab.get(word, 1) for word in summary.split()], dtype=torch.long) for summary in summaries]\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx][:self.max_len]\n",
    "        summary = self.summaries[idx][:self.max_len]\n",
    "        return text, summary"
   ],
   "id": "281313773b68967c",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Collate Function for Padding",
   "id": "e6cdc0247f60e74c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T04:06:16.131486Z",
     "start_time": "2025-03-01T04:06:16.126637Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MAX_LEN = 75  # Set a reasonable sequence length\n",
    "\n",
    "def collate_fn(batch):\n",
    "    text, summary = zip(*batch)  # Unpack batch\n",
    "    \n",
    "    text = [torch.as_tensor(t[:MAX_LEN], dtype=torch.long).clone().detach() for t in text]\n",
    "    summary = [torch.as_tensor(s[:MAX_LEN], dtype=torch.long).clone().detach() for s in summary]\n",
    "\n",
    "    text = pad_sequence(text, batch_first=True, padding_value=0)  # Pad sequences\n",
    "    summary = pad_sequence(summary, batch_first=True, padding_value=0)\n",
    "\n",
    "    return text, summary\n",
    "\n"
   ],
   "id": "8e86937561c703a9",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define LSTM Model",
   "id": "34c9484f74d5a31e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T04:06:17.069598Z",
     "start_time": "2025-03-01T04:06:17.063252Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LSTMSeq2Seq(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, embedding_matrix, dropout=0.3):\n",
    "        super(LSTMSeq2Seq, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        self.encoder = nn.LSTM(embed_size, hidden_size, batch_first=True, dropout=dropout)\n",
    "        self.decoder = nn.LSTM(embed_size, hidden_size, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
    "        embed_src = self.embedding(src)\n",
    "        embed_tgt = self.embedding(tgt)\n",
    "        \n",
    "        _, (hidden, cell) = self.encoder(embed_src)\n",
    "        \n",
    "        outputs = []\n",
    "        decoder_input = embed_tgt[:, 0].unsqueeze(1)  # Start token\n",
    "        \n",
    "        for t in range(tgt.shape[1] - 1):\n",
    "            output, (hidden, cell) = self.decoder(decoder_input, (hidden, cell))\n",
    "            output = self.fc(output)\n",
    "            outputs.append(output)\n",
    "\n",
    "            # Teacher forcing: Use true target word some of the time\n",
    "            if random.random() < teacher_forcing_ratio:\n",
    "                decoder_input = self.embedding(tgt[:, t + 1]).unsqueeze(1)\n",
    "            else:\n",
    "                decoder_input = self.embedding(torch.argmax(output, dim=-1)).detach()\n",
    "\n",
    "        return torch.cat(outputs, dim=1)\n"
   ],
   "id": "950a8877130b8868",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##  Function to generate summary",
   "id": "5dbd797747bb13c6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T04:06:18.674357Z",
     "start_time": "2025-03-01T04:06:18.666246Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import heapq\n",
    "import numpy as np\n",
    "\n",
    "def beam_search(model, text, vocab, rev_vocab, beam_width=3, max_len=50):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Convert text to indices\n",
    "        text_indices = [vocab.get(word, vocab[\"<UNK>\"]) for word in text.split()]\n",
    "        text_tensor = torch.tensor(text_indices, dtype=torch.long).unsqueeze(0).to(device)  # (1, seq_len)\n",
    "\n",
    "        # Encode input text\n",
    "        embedded_text = model.embedding(text_tensor)\n",
    "        _, (hidden, cell) = model.encoder(embedded_text)  # Encoder output states\n",
    "\n",
    "        # Initialize beam search: (score, sequence, hidden state, cell state)\n",
    "        sequences = [(0, [vocab[\"<SOS>\"]], hidden, cell)]\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            all_candidates = []\n",
    "            for score, seq, h, c in sequences:\n",
    "                last_word = seq[-1]\n",
    "\n",
    "                # Stop expanding sequences that already reached <EOS>\n",
    "                if last_word == vocab[\"<EOS>\"]:\n",
    "                    all_candidates.append((score, seq, h, c))\n",
    "                    continue\n",
    "\n",
    "                # Convert last word to tensor\n",
    "                last_word_tensor = torch.tensor([last_word], dtype=torch.long).to(device)\n",
    "                embedded_input = model.embedding(last_word_tensor).unsqueeze(1)  # (1, 1, embed_size)\n",
    "\n",
    "                # Decoder step\n",
    "                output, (new_h, new_c) = model.decoder(embedded_input, (h, c))\n",
    "\n",
    "                # Compute probabilities and get top-k words\n",
    "                output_probs = torch.softmax(model.fc(output.squeeze(1)), dim=-1)  # (1, vocab_size)\n",
    "                topk_probs, topk_indices = torch.topk(output_probs, beam_width, dim=-1)  # (1, beam_width)\n",
    "\n",
    "                # Add new candidates to the list\n",
    "                for i in range(beam_width):\n",
    "                    word_idx = topk_indices[0][i].item()\n",
    "                    word_prob = topk_probs[0][i].item()\n",
    "\n",
    "                    candidate = (score + np.log(word_prob), seq + [word_idx], new_h, new_c)\n",
    "                    all_candidates.append(candidate)\n",
    "\n",
    "            # Keep top `beam_width` sequences\n",
    "            sequences = sorted(all_candidates, key=lambda x: x[0], reverse=True)[:beam_width]\n",
    "\n",
    "        # Select the best sequence\n",
    "        best_seq = sequences[0][1]\n",
    "\n",
    "        # Convert indices to words\n",
    "        summary = \" \".join([rev_vocab[idx] for idx in best_seq if idx not in {vocab[\"<SOS>\"], vocab[\"<EOS>\"], vocab[\"<PAD>\"]}])\n",
    "        return summary\n"
   ],
   "id": "1cd008bc039281b1",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T04:06:27.739332Z",
     "start_time": "2025-03-01T04:06:27.732726Z"
    }
   },
   "cell_type": "code",
   "source": [
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "LR = 0.001"
   ],
   "id": "5251780891645be5",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Create DataLoaders",
   "id": "3b9a910a659cb04a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T04:06:32.022660Z",
     "start_time": "2025-03-01T04:06:29.123375Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "\n",
    "subset_size = 5000\n",
    "train_indices = list(range(min(subset_size, len(train_texts))))\n",
    "val_indices = list(range(min(subset_size, len(val_texts))))\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_dataset = NewsDataset(train_texts, train_summaries, vocab)\n",
    "val_dataset = NewsDataset(val_texts, val_summaries, vocab)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, collate_fn=collate_fn, pin_memory=True, num_workers=4, prefetch_factor=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, sampler=val_sampler, collate_fn=collate_fn, pin_memory=True, num_workers=4, prefetch_factor=2)"
   ],
   "id": "98c8636225b02a05",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T04:06:33.335511Z",
     "start_time": "2025-03-01T04:06:32.030086Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = LSTMSeq2Seq(VOCAB_SIZE, EMBED_SIZE, HIDDEN_SIZE, embedding_matrix).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=2)"
   ],
   "id": "56fafcb0f69fdc64",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/philiphousden/miniconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training loop",
   "id": "eabb3c885f4ed266"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T04:06:34.501815Z",
     "start_time": "2025-03-01T04:06:33.362070Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "gc.collect()  \n",
    "torch.cuda.empty_cache()\n",
    "model = torch.compile(model)  # JIT Compilation for speedup\n",
    "\n"
   ],
   "id": "efd9178b2dffd881",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T04:06:53.859306Z",
     "start_time": "2025-03-01T04:06:53.851763Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=10, patience=3):\n",
    "    scaler = torch.amp.GradScaler()\n",
    "    best_val_loss = float(\"inf\")\n",
    "    counter = 0\n",
    "    accumulation_steps = 4  # Accumulate gradients over 4 batches before updating\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        for i, (text, summary) in enumerate(train_loader):  # Use enumerate() here!\n",
    "            text, summary = text.to(device), summary.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.amp.autocast(\"cuda\", dtype=torch.float16):\n",
    "                output = model(text, summary)\n",
    "                loss = criterion(output.view(-1, VOCAB_SIZE).to(device), summary[:, 1:].reshape(-1).to(device))\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if (i + 1) % accumulation_steps == 0:  # Only update every N steps\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation Step\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for text, summary in val_loader:\n",
    "                text, summary = text.to(device), summary.to(device)\n",
    "                with torch.amp.autocast(\"cuda\", dtype=torch.float16):\n",
    "                    output = model(text, summary)\n",
    "                    loss = criterion(output.view(-1, VOCAB_SIZE), summary[:, 1:].reshape(-1))\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        # Early Stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            counter = 0\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(\"Early stopping triggered. Loading best model.\")\n",
    "                model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "                break\n"
   ],
   "id": "92b9bf81de1f8131",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-03-01T04:06:55.682581Z"
    }
   },
   "cell_type": "code",
   "source": "train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, EPOCHS)",
   "id": "56b00891c923729f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 10.4218, Val Loss: 9.4473\n",
      "Epoch 2, Train Loss: 9.3080, Val Loss: 9.3451\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T04:06:47.874107900Z",
     "start_time": "2025-02-28T16:58:48.495708Z"
    }
   },
   "cell_type": "code",
   "source": [
    "summary = beam_search(model, train_texts[0], vocab, rev_vocab, beam_width=5, max_len=75)\n",
    "print(\"Generated Summary:\", summary)\n"
   ],
   "id": "2e80a20793b4ae9b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Summary: was to to the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n"
     ]
    }
   ],
   "execution_count": 76
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
